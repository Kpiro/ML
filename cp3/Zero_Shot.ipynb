{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción al Zero-Shot Learning\n",
    "Zero-Shot Learning (ZSL) es una técnica que permite a los modelos de inteligencia artificial clasificar elementos de clases que nunca han sido vistas durante el entrenamiento. En lugar de aprender directamente de ejemplos etiquetados, el modelo utiliza información semántica, como descripciones textuales, atributos o relaciones entre clases, para realizar predicciones.\n",
    "\n",
    "Esto representa un gran avance frente al aprendizaje supervisado tradicional, ya que elimina la necesidad de recopilar y etiquetar datos específicos para cada nueva clase.\n",
    "\n",
    "### Ejemplo Conceptual\n",
    "Imagina un modelo entrenado para reconocer imágenes de perros, gatos y aves. Si le mostramos una imagen de un caballo (una clase que nunca ha visto), el modelo puede clasificarla correctamente si tiene acceso a una descripción textual como:\n",
    "\"Un animal con cuatro patas, crin y que suele ser usado para montar.\"\n",
    "\n",
    "En este caso, el modelo utiliza la descripción como referencia para encontrar similitudes con la imagen, logrando identificar la clase desconocida.\n",
    "\n",
    "### CLIP\n",
    "Para ilustrar un ejemplo de Zero-Shot Learning, haremos uso de CLIP. CLIP (Contrastive Language-Image Pretraining) es un modelo desarrollado por OpenAI que representa un avance importante en Zero-Shot Learning. Su capacidad principal es alinear representaciones de imágenes y textos en un espacio latente compartido. Esto significa que CLIP puede entender la relación entre una descripción textual y una imagen, incluso si no ha sido entrenado directamente en la clase que se analiza.\n",
    "\n",
    "Está compuesto por dos partes principales:\n",
    "\n",
    "Un codificador de imágenes: Convierte una imagen en un vector numérico (embedding).\n",
    "Un codificador de texto: Convierte una descripción textual en otro vector numérico.\n",
    "Ambos codificadores transforman sus entradas a un espacio latente compartido, donde la similitud entre las representaciones se maximiza para pares correctos (imagen-descripción) y se minimiza para pares incorrectos. Por ejemplo:\n",
    "\n",
    "La representación de una imagen de un \"perro\" estará más cerca del texto \"una foto de un perro\" que de \"una foto de un gato\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-Shot Learning con CLIP\n",
    "\n",
    "# ================================\n",
    "# Paso 1: Instalación de librerías necesarias\n",
    "# ================================\n",
    "# Asegúrate de tener las siguientes librerías instaladas antes de comenzar.\n",
    "# Ejecuta este comando si no las tienes instaladas.\n",
    "\n",
    "%pip install transformers torch pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Paso 2: Importación de librerías\n",
    "# ================================\n",
    "# Importamos las librerías necesarias para trabajar con el modelo CLIP y procesar imágenes.\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Paso 3: Cargar el modelo CLIP pre-entrenado\n",
    "# ================================\n",
    "# Descargamos el modelo preentrenado CLIP desde el hub de modelos de OpenAI.\n",
    "\n",
    "modelo = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "procesador = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Paso 4: Cargar una imagen\n",
    "# ================================\n",
    "# Cargamos una imagen local para realizar la clasificación. Cambia la ruta para usar tu propia imagen.\n",
    "\n",
    "imagen = Image.open(\"/content/dog.jpg\")  # Reemplaza con la ruta de tu imagen\n",
    "imagen.show()  # Muestra la imagen cargada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Paso 5: Definir etiquetas de clase\n",
    "# ================================\n",
    "# Creamos una lista de descripciones textuales que representan las clases a clasificar.\n",
    "\n",
    "etiquetas = [\"una foto de un perro\", \"una foto de un gato\", \"una foto de un coche\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Paso 6: Preprocesar las entradas\n",
    "# ================================\n",
    "# Convertimos la imagen y las etiquetas en un formato que el modelo CLIP pueda procesar.\n",
    "\n",
    "entradas = procesador(text=etiquetas, images=imagen, return_tensors=\"pt\", padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Paso 7: Clasificación Zero-Shot\n",
    "# ================================\n",
    "# Realizamos la clasificación utilizando el modelo CLIP y obtenemos las puntuaciones de similitud.\n",
    "\n",
    "salidas = modelo(**entradas)\n",
    "logits_por_imagen = salidas.logits_per_image  # Puntuaciones de similitud\n",
    "probabilidades = logits_por_imagen.softmax(dim=1)  # Distribución de probabilidades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Paso 8: Obtener la clase predicha\n",
    "# ================================\n",
    "# Identificamos la clase con mayor probabilidad.\n",
    "\n",
    "clase_predicha = etiquetas[probabilidades.argmax()]\n",
    "print(f\"Clase predicha: {clase_predicha}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
